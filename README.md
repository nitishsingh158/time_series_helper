# AI Assistant for Time Series Analysis

A chat-based AI assistant that enables managers to easily analyze IoT time series data from mechanical equipment via API.

## Features

-   Connect to IoT data APIs for real-time data access
-   Intelligent caching for performance optimization
-   Calculate basic statistics (min, max, average, etc.)
-   Chat-based interface for natural language queries
-   Machine status monitoring and data retrieval

## Current System Architecture

```
Frontend (Streamlit)
â”œâ”€â”€ Chat Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ GraphChatAgent (LangGraph)
â””â”€â”€ Visualization Panel                â”‚
                                       â–¼
API Data Access Layer â”€â”€â”€â”€â”€â”€â”€â”€ IoT Data APIs
â”œâ”€â”€ APIConnector                       â”‚
â”œâ”€â”€ CacheManager                       â”‚
â””â”€â”€ MetadataManager                    â”‚
â”œâ”€â”€ Anomaly Detection                  â”‚
â”œâ”€â”€ Seasonality Detection              â”‚
â”œâ”€â”€ Basic Statistics                   â”‚
â””â”€â”€ Data Preprocessing                 â”‚
                                       â–¼
Data Layer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API Data Access (To Be Built)
â”œâ”€â”€ CSV Loader (Current)
â””â”€â”€ API Connectors (Planned)
```

#### GraphChatAgent (âœ… Implemented)

**Purpose**: Natural language processing and conversation management

-   **Supervisor Pattern**: Intent classification and workflow routing
-   **Message Processing**: Clarification and rewriting of ambiguous requests
-   **Tool Orchestration**: Coordination of information gathering tools
-   **Response Generation**: Natural language response synthesis

#### AnalysisAgent (ğŸš§ To Be Built)

**Purpose**: Execution and orchestration of time series analysis

-   **Analysis Planning**: Decompose complex requests into specific analysis tasks
-   **Tool Orchestration**: Coordinate multiple analysis types (statistics, anomalies, etc.)
-   **Result Synthesis**: Combine and interpret results from multiple analyses
-   **Quality Validation**: Ensure analysis completeness and statistical significance

#### Data Access Layer (ğŸš§ To Be Built)

**Purpose**: Unified interface for data retrieval and management

-   **API Connectors**: Batch processing from various data sources
-   **Data Validation**: Ensure data quality and completeness
-   **Metadata Management**: Track data schemas, time ranges, and source information
-   **Caching**: Performance optimization for repeated requests

## Analysis Workflow

### User Request Processing Flow

```
1. User Query (Natural Language)
   â†“
2. GraphChatAgent
   â”œâ”€â”€ Intent Classification
   â”œâ”€â”€ Message Clarification (if needed)
   â””â”€â”€ Analysis Request Preparation
   â†“
3. AnalysisAgent
   â”œâ”€â”€ Analysis Planning & Decomposition
   â”œâ”€â”€ Data Retrieval (via Data Access Layer)
   â”œâ”€â”€ Multi-Analysis Execution
   â””â”€â”€ Result Synthesis
   â†“
4. GraphChatAgent
   â”œâ”€â”€ Response Generation
   â””â”€â”€ Visualization Coordination
   â†“
5. Frontend Display
   â”œâ”€â”€ Natural Language Response
   â”œâ”€â”€ Interactive Charts
   â””â”€â”€ Data Tables
```

### Analysis Coverage Strategy

#### Progressive Analysis Pattern

-   **Level 1**: Quick Overview (basic stats, data validation, obvious patterns)
-   **Level 2**: Targeted Analysis (anomalies, seasonality, specific measurements)
-   **Level 3**: Deep Dive (comprehensive multi-column analysis, correlations)

#### Analysis Taxonomy

```
Time Series Analysis Capabilities:
â”œâ”€â”€ Data Validation
â”‚   â”œâ”€â”€ Missing data detection
â”‚   â”œâ”€â”€ Data quality assessment
â”‚   â””â”€â”€ Time continuity validation
â”œâ”€â”€ Descriptive Analytics
â”‚   â”œâ”€â”€ Basic statistics (min, max, mean, std, median)
â”‚   â”œâ”€â”€ Distribution analysis
â”‚   â””â”€â”€ Summary reports
â”œâ”€â”€ Diagnostic Analytics
â”‚   â”œâ”€â”€ Anomaly detection (single and multi-column)
â”‚   â”œâ”€â”€ Outlier identification
â”‚   â””â”€â”€ Pattern deviation analysis
â”œâ”€â”€ Temporal Analytics
â”‚   â”œâ”€â”€ Seasonality detection
â”‚   â”œâ”€â”€ Trend analysis
â”‚   â””â”€â”€ Cycle identification
â””â”€â”€ Comparative Analytics
    â”œâ”€â”€ Multi-column correlations
    â”œâ”€â”€ Cross-variable analysis
    â””â”€â”€ Benchmark comparisons
```

## Technology Stack

-   **Frontend**: Streamlit for interactive web interface
-   **Backend**: Python with pandas, numpy, scipy for data processing
-   **LLM Integration**: LangChain with Google Gemini for natural language processing
-   **Workflow Management**: LangGraph for agent orchestration
-   **Visualization**: Plotly for interactive charts
-   **Data Processing**: Asynchronous batch processing

### Setup Instructions

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd ai_assitant_tsa
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Environment Configuration**
   ```bash
   # Copy the example environment file
   cp .env.example .env
   
   # Edit .env file with your API keys
   nano .env
   ```
   
   Required environment variables in `.env`:
   - `GOOGLE_API_KEY`: Your Google Gemini API key
   - `LANGSMITH_API_KEY`: Your LangSmith API key (optional, for tracing)
   - `OPENAI_API_KEY`: Your OpenAI API key (if using OpenAI models)

4. **Run the application**
   ```bash
   # Start the data server (in one terminal)
   python data_gen/main.py
   
   # Start the frontend (in another terminal)
   streamlit run app.py
   ```

âš ï¸ **Security Note**: Never commit your `.env` file with real API keys to version control.

### Usage Examples

-   "Show me anomalies in the temperature and pressure data"
-   "What are the basic statistics for all sensors over the last week?"
-   "Is there a daily pattern in the equipment vibration data?"
-   "Validate the data quality and identify any missing values"
-   "Create a comprehensive analysis dashboard for equipment monitoring"
